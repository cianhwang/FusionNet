{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "#from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(y, nb_channels, _strides = (1,1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.ReLU()(y)\n",
    "\n",
    "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = layers.BatchNormalization()()\n",
    "\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        shortcut = layers.Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = layers.add([shortcut, y])\n",
    "    #y = layers.LeakyReLU()(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net(x, nb_channels, _strides=(1, 1)):\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(x)\n",
    "    shortcut = x\n",
    "    for _ in range(16):\n",
    "        x = res_block(x, 64)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(x)\n",
    "    x = layers.add([shortcut, x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, nb_channels, _strides=(1, 1)):\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(x)\n",
    "    #x = layers.Conv2D(64, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_net(y, nb_channels, _strides=(1, 1)):\n",
    "    #y = layers.Conv2D(64, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(y)\n",
    "    #y = layers.Conv2D(32, kernel_size=(3, 3), strides=_strides, padding='same', activation='relu')(y)\n",
    "    y = layers.Conv2D(3, kernel_size=(3, 3), strides=_strides, padding='same', activation='linear')(y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_imgs(path, number, train_type):\n",
    "    result=np.empty((number, 64, 64, 3), dtype=\"float64\")\n",
    "    for i in range(number):\n",
    "        I = cv2.imread(path + \"{:04}_{}.jpeg\".format(i+1, train_type))\n",
    "        result[i, :, :, :] = I\n",
    "    return result/result.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inport training data\n",
    "dataNum = 1000\n",
    "x1_train = load_imgs(\"./blurImg/\", dataNum, 1)\n",
    "x2_train = load_imgs(\"./blurImg/\", dataNum, 2)\n",
    "y_train = load_imgs(\"./blurImg/\", dataNum, 0)\n",
    "\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "        \n",
    "def loss_wrapper(in_tensor1, in_tensor2):\n",
    "    def gaussian_blur(in_tensor):\n",
    "        # use large kernel to blur pred and in_tensor//\n",
    "        return\n",
    "        \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        # or better implementation like fourier transformation\n",
    "        return K.binary_crossentropy(y_true, y_pred) + K.reduce_mean(K.square(gaussian_blur(y_pred)-gaussian_blur(in_tensor1)))\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "img_a = layers.Input(shape=(64, 64, 3))\n",
    "img_b = layers.Input(shape=(64, 64, 3))\n",
    "#feature_a = conv_net(img_a, 3)\n",
    "#feature_b = conv_net(img_b, 3)\n",
    "feature_a = res_net(img_a, 3)\n",
    "feature_b = res_net(img_b, 3)\n",
    "merge = layers.concatenate([feature_a, feature_b])\n",
    "aif = post_net(merge, 128)\n",
    "gen = Model(inputs = [img_a, img_b], outputs = [aif])\n",
    "#gen.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gen.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "#gen.summary()\n",
    "#plot_model(gen, to_file='generator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fake = gen([img_a, img_b])\n",
    "dis = Sequential()\n",
    "dis.add(layers.Conv2D(64, kernel_size=(3, 3),strides=(2, 2), padding='same'))\n",
    "dis.add(layers.LeakyReLU())\n",
    "#dis.add(layers.Dropout(0.25))\n",
    "dis.add(layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2),padding='same'))\n",
    "dis.add(layers.LeakyReLU())\n",
    "#dis.add(layers.Dropout(0.25))\n",
    "dis.add(layers.Conv2D(256, kernel_size=(3, 3), strides=(2, 2),padding='same'))\n",
    "dis.add(layers.LeakyReLU())\n",
    "#dis.add(layers.Dropout(0.25))\n",
    "#dis.add(layers.Conv2D(1, kernel_size=(3, 3), padding='same'))\n",
    "\n",
    "dis.add(layers.Flatten())\n",
    "dis.add(layers.Dense(256))\n",
    "dis.add(layers.Dense(2))\n",
    "dis.add(layers.Activation('softmax'))\n",
    "pred_prob = dis(image_fake)\n",
    "dis.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#dis.summary()\n",
    "#plot_model(dis, to_file='discriminator.png')\n",
    "make_trainable(dis, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 64, 64, 3)    2444291     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 2)            4565890     model_1[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,010,181\n",
      "Trainable params: 2,444,291\n",
      "Non-trainable params: 4,565,890\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "am = Model(inputs = [img_a, img_b], outputs = [pred_prob])\n",
    "am.summary()\n",
    "am.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#plot_model(am, to_file='adversary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(losses[\"d\"], label='discriminitive loss')\n",
    "        plt.plot(losses[\"g\"], label='generative loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.5539 - acc: 0.7500\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 3s 194ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 2.1040 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 15.8622 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0591 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 8.0591 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 710us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 8.0591 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 8.0598 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 764us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 761us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 781us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 713us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 839us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 778us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 813us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 837us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 793us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 763us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1921e-07 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train discriminator on generated images\n",
    "losses = {\"d\":[], \"g\":[]}\n",
    "Batch_size = 16\n",
    "nb_epoch = 100\n",
    "for epoch in range(nb_epoch):\n",
    "    rand_idx = np.random.randint(0, x1_train.shape[0], size = Batch_size)\n",
    "    img_batch1 = x1_train[rand_idx, :, :, :]\n",
    "    img_batch2 = x2_train[rand_idx, :, :, :]\n",
    "    y_batch = y_train[np.random.randint(0, y_train.shape[0], size = Batch_size), :, :, :]\n",
    "    #gen.fit([x1_train, x2_train], y_train)\n",
    "\n",
    "    \n",
    "    gen_img = gen.predict([img_batch1, img_batch2])\n",
    "    X = np.concatenate((y_batch, gen_img))\n",
    "    y = np.zeros([2*Batch_size,2])\n",
    "    y[0:Batch_size, 1] = 1\n",
    "    y[Batch_size:, 0] = 1\n",
    "    make_trainable(dis,True)\n",
    "    dis.fit(X, y,epochs=1, batch_size=Batch_size*2)\n",
    "    #losses[\"d\"].append(d_loss)\n",
    " \n",
    "    y2 = np.zeros([Batch_size, 2])\n",
    "    y2[:, 1] = 1\n",
    "    # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "    make_trainable(dis,False)\n",
    "    am.fit([img_batch1, img_batch2], y2,epochs=1, batch_size=Batch_size) #same batch or ???\n",
    "    #losses[\"g\"].append(g_loss)\n",
    "    #if epoch % 25 == 25 - 1:\n",
    "    #    plot_loss(losses)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 1000\n",
    "\n",
    "rand_idx = np.random.randint(0, x1_train.shape[0], size = Batch_size)\n",
    "img_batch1 = x1_train[rand_idx, :, :, :]\n",
    "img_batch2 = x2_train[rand_idx, :, :, :]\n",
    "y_batch = y_train[rand_idx, :, :, :]\n",
    "gen.fit([img_batch1, img_batch2], y_batch)\n",
    "    \n",
    "\n",
    "gen_img[0]\n",
    "gen_img.min()\n",
    "gen_img = gen.predict([x1_train, x2_train])    \n",
    "cv2.imwrite(\"a.jpg\", 255.0*(gen_img[4]-gen_img[4].min())/(gen_img[4].max()-gen_img[4].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-train discriminate network\n",
    "make_trainable(dis,True)\n",
    "Batch_size = 100\n",
    "\n",
    "rand_idx = np.random.randint(0, x1_train.shape[0], size = Batch_size)\n",
    "img_batch1 = x1_train[rand_idx, :, :, :]\n",
    "img_batch2 = x2_train[rand_idx, :, :, :]\n",
    "y_batch = y_train[np.random.randint(0, y_train.shape[0], size = Batch_size), :, :, :]\n",
    "gen_img = gen.predict([img_batch1, img_batch2])\n",
    "X = np.concatenate((y_batch, gen_img))\n",
    "y = np.zeros([2*Batch_size,2])\n",
    "y[0:Batch_size, 0] = 1\n",
    "y[Batch_size:, 1] = 1\n",
    "\n",
    "dis.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
